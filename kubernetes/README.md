# Kubernetes

Kubernetes (K8s)は、コンテナ化されたアプリケーションのデプロイ、スケーリング、管理を自動化するためのオープンソースのプラットフォーム。
コンテナ化されたアプリケーションを運用する上での多くの課題を解決し、信頼性の高いスケーラブルなシステムを構築するための強力なツール。これにより、アプリケーションのデプロイメントや管理にかかる時間と労力を大幅に削減することができる。

Google によって開発された後、現在は Cloud Native Computing Foundation (CNCF) がプロジェクトをホストしている。

## 基本概念

1. **コンテナ**:

   - コンテナは、アプリケーションとその依存関係を一緒にパッケージ化したもの。
   - Docker が代表的なコンテナ技術として使用されることが多い

2. **Pod**:

   - Kubernetes の基本的なデプロイメントユニット。
   - 1 つ以上のコンテナを含むことができ、これらのコンテナは同じネットワークとストレージを共有する

3. **Node**:

   - Kubernetes クラスター内の個々のマシン。
   - `マスターノード(control plane)`と`ワーカーノード`がある

4. **Cluster**:
   - 動作する一連のノードの集まり。複数のマスターとワーカーが含まれる場合がある

## 制御面 (Control Plane)

1. **etcd**:

   - 分散キーバリューストアで、全てのクラスターデータを保存・管理する

2. **Kube-apiserver**:

   - Kubernetes API を提供し、クラスターの全てのコンポーネントとのやり取りを行う

3. **Kube-scheduler**:

   - 新しく作成された Pod を適切なノードに割り当てる

4. **Kube-controller-manager**:
   - 各種コントローラーを実行し、クラスタの状態を望ましい状態に保つ

## 作業部面 (Worker Nodes)

1. **Kubelet**:

   - 各ノードで動作し、ポッドが適切に実行されていることを確認する

2. **Kube-proxy**:

   - 各ノードで動作し、ネットワークプロキシを提供し、サービスのロードバランシングを行う

3. **Container Runtime**:
   - 実際にコンテナを実行するソフトウェア（例: Docker）

## 主要機能

1. **自動化されたデプロイとスケーリング**:

   - Kubernetes は自動でアプリケーションのデプロイやスケーリングを行う

2. **自己回復**:

   - Pod が失敗すると自動的に再起動し、ノードが死んだ場合は他のノードに Pod を再スケジュールする

3. **デプロイメントストラテジー**:

   - ローリングアップデートやカナリアデプロイなどの様々なデプロイメント戦略をサポートする

4. **サービスディスカバリーとロードバランシング**:

   - サービスの IP アドレスと DNS 名を自動的に割り当て、負荷分散を行う

5. **ストレージオーケストレーション**:

   - 任意のストレージシステムを自動的にマウントし、アプリケーションに提供できる

6. **シークレットと設定管理**:
   - シークレットやアプリケーションの設定を安全に保存し、Pod に提供する

## Kubernetes を使用しなくても可能なこと

1. **基本的なインフラ構築**:

   - 単一の VM やオートスケーリンググループを使ってアプリケーションをホストする。
   - データベースやストレージの設定。
   - 適切なネットワーク設定。

2. **コンテナのデプロイ**:

   - コンテナは単独の VM や Amazon ECS（AWS のコンテナサービス）、Google Cloud Run などを使って実行可能。

3. **シンプルなスケーリング**:
   - オートスケーリンググループやインスタンスタイプのスケールアップ/ダウン。

Kubernetes を使用しなくても、多くのシステムは`AWS`や`GCP`上で問題なくデプロイすることができる。より高度なデプロイメント戦略や運用能力を求める場合、Kubernetes は非常に有用。プロジェクトの規模や要件に応じて慎重に判断することが重要。

Kubernetes は、特にマイクロサービスアーキテクチャや複雑なデプロイメントが必要なケースで、より高度な管理とオーケストレーションを提供するが、必ずしも必須ではない。

## Kubernetes を使用することで得られるメリット

1. **複雑なアーキテクチャの管理**:

   - マイクロサービスアーキテクチャの運用が容易。
   - 複数のサービスの相互依存関係を管理。

2. **高い可用性とフェールオーバー**:

   - Pod の自己回復機能により、自動的に障害から復旧。
   - ノードの障害時に Pod を他のノードに再スケジューリング。

3. **自動スケーリングとリソース効率**:

   - Kubernetes オートスケール（Horizontal Pod Autoscaler）を使った自動スケーリング。
   - リソース使用率に応じた動的なスケーリングとスケジューリング。

4. **宣言的なインフラ管理**:

   - インフラをコード（IaC）として宣言的に管理。
   - 望ましい状態を定義して、Kubernetes がその状態を維持。

5. **マルチクラウドおよびハイブリッドクラウドのサポート [これが唯一かもしれない]**:

   - 複数のクラウドプロバイダ間でのシームレスなデプロイが可能。
   - オンプレミスおよびクラウド間での一貫性ある運用。

6. **シークレットと設定の管理**:

   - セキュアな方法でシークレットや設定を管理し、Pod に提供可能。

7. **サービスディスカバリーとロードバランシング**:
   - 内部および外部のサービスディスカバリー。
   - 自動的な負荷分散。

### AWS での高可用性とフェールオーバー

高い可用性とフェールオーバーは、`Kubernetes`を使用しなくても`AWS`や`GCP`で実現できる。それぞれのクラウドプロバイダが提供するネイティブな機能やサービスを利用することで、同様に高可用性とフェールオーバーを実現することが可能。

- [AWS での高可用性とフェールオーバー](../cloud/aws/high-availability.md)
- [GCP での高可用性とフェールオーバー](../cloud/gcp/high-availability.md)

### 自動スケーリングとリソース効率

自動スケーリングとリソース効率は、`Kubernetes`を使用しなくても`AWS`や`GCP`で十分に実現できる。これらのクラウドプロバイダーは、それぞれ強力なネイティブツールとサービスを提供しており、Kubernetes を使わずとも高いレベルでスケーリングとリソース効率を達成できる。

- [AWS での自動スケーリングとリソース効率の実現方法](../cloud/aws/auto-scaling.md)
- [GCP での自動スケーリングとリソース効率の実現方法](../cloud/gcp/auto-scaling.md)

## Kubernetes を使うメリットが得られる可能性がある具体的なユースケース

1. **マイクロサービスアーキテクチャ**:

   - 多くの小さな、相互に依存するサービスを持つ場合、Kubernetes の管理能力が大きく役立つ。

2. **高可用性**:

   - ミッションクリティカルなアプリケーションでダウンタイムを最小限に抑えたい場合。

3. **大規模デプロイメント**:

   - 多数のリソースを管理する大規模なシステムやアプリケーション。

4. **継続的デリバリー(CD)パイプライン**:
   - 継続的デプロイメントと継続的インテグレーションが重要なシステム。

## デメリット

- Kubernetes を導入する学習コスト
- セットアップのコスト

## References

- [終わらせましょう。複雑過ぎる Kubernetes／クラウドネイティブが生む心理的安全性の低下を――無料でクラウドセキュリティの勘所が分かる 130 ページの電子書籍](https://atmarkit.itmedia.co.jp/ait/articles/2412/27/news090.html)
