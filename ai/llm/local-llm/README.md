# Local LLM

ローカル LLM（Local Large Language Model）は、クラウド環境ではなく、ユーザーが所有するローカル環境（PC、オンプレミスサーバー、スマートフォン、エッジデバイスなど）で直接動作する大規模言語モデル。この技術により、データをクラウドに送信せずにローカル環境内で処理することが可能になる。

## ローカル LLM の主な特徴

1. **データプライバシーの保護**: 外部サーバーにデータを送信しないため、機密情報の漏洩リスクを低減できる。

2. **オフライン利用**: インターネット接続なしで動作するため、場所を問わず利用可能。

3. **カスタマイズ性**: 特定の用途や業界に合わせたモデル調整が可能。

4. **コスト効率**: クラウドサービスの利用料を削減でき、長期的な運用コストの低減が可能。

5. **リアルタイム処理**: データをデバイス内で直接処理するため、即時の応答が求められる場面でも迅速に対応できる。

## ローカル LLM の活用事例

ローカル LLM は以下のような場面で特に有効

- 企業の機密情報を扱う場面
- インターネット接続が制限される環境
- リアルタイム性が求められる場合
- 大量のデータ処理が必要な場合

## ローカル LLM の課題

ローカル LLM の導入には以下のような課題もある

- 高性能なハードウェアが必要になる場合がある
- モデルの更新や管理が必要
- 大規模なモデルの場合、処理速度が遅くなる可能性がある

ローカル LLM は、従来のクラウドベース AI とは異なる特性を持ち、新たな AI 活用の可能性を切り開く技術として注目されている。データプライバシーの保護やカスタマイズ性の高さから、今後さまざまな分野での活用が期待されている。

## Tools

1. **Llama.cpp**
   - 軽量で効率的なコマンドラインツール。
   - Mistral、Falconなどのさまざまなモデルをサポート。
   - GPUとCPUの両方で動作し、リソースが限られたセットアップに適している

2. **LM Studio**
   - モデルを管理するためのユーザーフレンドリーなグラフィカルインターフェイス。
   - 簡単にカスタマイズでき、さまざまなLLMをサポートしている

3. **Ollama**
   - 迅速な展開のためにパッケージ化されたモデルを提供する。
   - マルチモーダル機能とシンプルなセットアッププロセス。

4. **GPT4All**
   - コンシューマーグレードのハードウェア向けに設計されたローカルファーストのエコシステム。
   - 強力なパフォーマンスを備えた直感的なインターフェイスを提供する。

5. **Faraday.dev**
   - 高度なカスタマイズと実験的なアーキテクチャに重点を置いている。
   - 最先端の AI プロジェクトに取り組む研究者に最適。

6. **local.ai**
   - 幅広い互換性と強力なコミュニティ サポートを備えた汎用プラットフォーム。

7. **Runpod**
   - コンテナ化された環境に LLM を展開するためのスケーラブルなソリューション。
   - 柔軟性とスケーラビリティを必要とするエンタープライズ ユーザーに適している。

## Models

### [Falcon 3](https://huggingface.co/blog/falcon3)

- Abu Dhabの Technology Innovation Institute (TII) が開発
- サイズ: 1B、3B、7B、10B パラメータ
- 14 兆トークンでトレーニング済み
- 多言語タスク (英語、フランス語、スペイン語、ポルトガル語) をサポート
- 最大 32k トークンの拡張コンテキスト ウィンドウを提供 (8k の 1B モデルを除く)

### [Gemma 3](https://developers.googleblog.com/en/introducing-gemma3/)

- Google の最新のオープンソース LLM ファミリー
- サイズ: 2B、9B、27B パラメータ
- Gemini モデルと同じ研究に基づいて構築
- さまざまなハードウェアで効率的な推論を行うように最適化
- 組み込みの安全性向上と責任ある AI プラクティスが含まれている

### [Llama 3.1, 3.2, 3.3](https://www.llama.com/)

- Meta (旧 Facebook) が開発
- Llama シリーズの改良版
-汎用性とパフォーマンスで知られている

### [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b)

- ローカル展開に人気の選択肢
- パフォーマンスとリソース要件のバランスが良好

### [GPT4All](https://www.nomic.ai/gpt4all)

- コンシューマーグレードのハードウェア向けに設計
- 標準 CPU で効率的に実行できる事前トレーニング済みモデル

### [DeepSeek-V3 / DeepSeek-R1](https://www.deepseek.com/)

- 最近のベンチマークで有望なパフォーマンスを示した新しいモデル

### [QVQ-72B-Preview](https://huggingface.co/Qwen/QVQ-72B-Preview)

- LLM 評価で比較された別の最新モデル

## References

- [ローカル LLM の小説生成能力を評価する：中規模モデル編](https://note.com/kohya_ss/n/n07f550125282)
- [2025 年さらに熱くなるであろうローカル LLM のための m4mac 購入検討](https://zenn.dev/afk2777/articles/localllm-mac)
- [ローカル LLM の実行ツール「Ollama」の GUI フロントエンド ⁠⁠⁠⁠、Alpaca を使用して生成 AI を使用する［Radeon 編］](https://gihyo.jp/admin/serial/01/ubuntu-recipe/0849)
- [ローカルLLMで遊ぶためにVRAM40GBのPCを構築したので、その顛末を記録する(計40万円)](https://www.robotech-note.com/entry/2025/02/28/%E3%83%AD%E3%83%BC%E3%82%AB%E3%83%ABLLM%E3%81%A7%E9%81%8A%E3%81%B6%E3%81%9F%E3%82%81%E3%81%ABVRAM40GB%E3%81%AEPC%E3%82%92%E5%B0%8E%E5%85%A5%E3%81%97%E3%81%9F%E3%81%AE%E3%81%A7%E3%80%81%E3%81%9D%E3%81%AE)
- [Gemma3をローカル環境のDockerで動かす（Ollama+OpenWebUI）](https://zenn.dev/takoyaki3/articles/f181f455ac21f7)
